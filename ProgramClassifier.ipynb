{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "vPykujsrUD_e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from spellchecker import SpellChecker\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "computer science                       405\n",
              "undeclared engineering                 359\n",
              "life science                           185\n",
              "business administration                175\n",
              "biomedical engineering                 131\n",
              "engineering science                    129\n",
              "nuclear engineering                    129\n",
              "commerce                               122\n",
              "management                             119\n",
              "software engineering                   116\n",
              "math                                   115\n",
              "cs/bba                                 112\n",
              "computer engineering                    99\n",
              "accounting                              93\n",
              "physics                                 93\n",
              "nanotechnology engineering              91\n",
              "arts/business                           90\n",
              "environmental science                   85\n",
              "biochemistry                            84\n",
              "physical sciences                       83\n",
              "communications                          82\n",
              "management engineering                  77\n",
              "bba/finmath                             73\n",
              "education                               72\n",
              "biomedical science                      70\n",
              "kinesiology                             69\n",
              "systems design engineering              68\n",
              "business technology managment           66\n",
              "computing and financial management      64\n",
              "psychology                              63\n",
              "health science                          62\n",
              "game design                             60\n",
              "industrial engineering                  57\n",
              "mineral engineering                     57\n",
              "mechatronics engineering                54\n",
              "civil engineering                       54\n",
              "math/bba                                49\n",
              "public administration                   48\n",
              "mechanical engineering                  47\n",
              "environmental engineering               47\n",
              "social sciences                         46\n",
              "aerospace engineering                   44\n",
              "dev degree                              44\n",
              "public affairs                          44\n",
              "planning                                40\n",
              "materials engineering                   36\n",
              "medical radiation sciences              35\n",
              "public health                           33\n",
              "chemical engineering                    32\n",
              "arts                                    31\n",
              "cs/math/stats                           29\n",
              "economics                               28\n",
              "biology                                 27\n",
              "medical science                         26\n",
              "architectural engineering               24\n",
              "humanities                              23\n",
              "nursing                                 20\n",
              "geomatics                               19\n",
              "pharmacy                                18\n",
              "accounting and financial management     16\n",
              "forestry                                16\n",
              "architecture                            15\n",
              "electrical engineering                  14\n",
              "sport management                        13\n",
              "law                                     11\n",
              "political science                        5\n",
              "political science/law                    3\n",
              "global business and digital arts         2\n",
              "oenology and viticulture                 1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"admissionData.csv\")\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "-xRnr9wgU0hj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accounting': 0,\n",
              " 'accounting and financial management': 1,\n",
              " 'aerospace engineering': 2,\n",
              " 'architectural engineering': 3,\n",
              " 'architecture': 4,\n",
              " 'arts': 5,\n",
              " 'arts/business': 6,\n",
              " 'bba/finmath': 7,\n",
              " 'biochemistry': 8,\n",
              " 'biology': 9,\n",
              " 'biomedical engineering': 10,\n",
              " 'biomedical science': 11,\n",
              " 'business administration': 12,\n",
              " 'business technology managment': 13,\n",
              " 'chemical engineering': 14,\n",
              " 'civil engineering': 15,\n",
              " 'commerce': 16,\n",
              " 'communications': 17,\n",
              " 'computer engineering': 18,\n",
              " 'computer science': 19,\n",
              " 'computing and financial management': 20,\n",
              " 'cs/bba': 21,\n",
              " 'cs/math/stats': 22,\n",
              " 'dev degree': 23,\n",
              " 'economics': 24,\n",
              " 'education': 25,\n",
              " 'electrical engineering': 26,\n",
              " 'engineering science': 27,\n",
              " 'environmental engineering': 28,\n",
              " 'environmental science': 29,\n",
              " 'forestry': 30,\n",
              " 'game design': 31,\n",
              " 'geomatics': 32,\n",
              " 'global business and digital arts': 33,\n",
              " 'health science': 34,\n",
              " 'humanities': 35,\n",
              " 'industrial engineering': 36,\n",
              " 'kinesiology': 37,\n",
              " 'law': 38,\n",
              " 'life science': 39,\n",
              " 'management': 40,\n",
              " 'management engineering': 41,\n",
              " 'materials engineering': 42,\n",
              " 'math': 43,\n",
              " 'math/bba': 44,\n",
              " 'mechanical engineering': 45,\n",
              " 'mechatronics engineering': 46,\n",
              " 'medical radiation sciences': 47,\n",
              " 'medical science': 48,\n",
              " 'mineral engineering': 49,\n",
              " 'nanotechnology engineering': 50,\n",
              " 'nuclear engineering': 51,\n",
              " 'nursing': 52,\n",
              " 'oenology and viticulture': 53,\n",
              " 'pharmacy': 54,\n",
              " 'physical sciences': 55,\n",
              " 'physics': 56,\n",
              " 'planning': 57,\n",
              " 'political science': 58,\n",
              " 'political science/law': 59,\n",
              " 'psychology': 60,\n",
              " 'public administration': 61,\n",
              " 'public affairs': 62,\n",
              " 'public health': 63,\n",
              " 'social sciences': 64,\n",
              " 'software engineering': 65,\n",
              " 'sport management': 66,\n",
              " 'systems design engineering': 67,\n",
              " 'undeclared engineering': 68}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encode programs as numbers and dictionarys to convert between them\n",
        "label2num = {label: num for num, label in enumerate(sorted(df['label'].unique()))}\n",
        "num2label = {num: label for label, num in label2num.items()}\n",
        "\n",
        "# Lists containing the programs and their labels\n",
        "programs = df['program'].tolist()\n",
        "labels = [label2num[label] for label in df['label'].tolist()]\n",
        "\n",
        "label2num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "4VclmIZzT0ww"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class ProgramClassificationDataset(Dataset):\n",
        "    def __init__(self, programs, labels, tokenizer, max_length):\n",
        "        self.programs = programs\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.programs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        program = self.programs[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(program, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)  # Tokenization (encoding is a step before word embeddings)\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create classifier architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "HbcluGnkWoaY"
      },
      "outputs": [],
      "source": [
        "# Classifier architecture\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)          # BERT abstraction \"layer\"\n",
        "        self.dropout = nn.Dropout(0.1)                                  # 10% Dropout Layer\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)  # Fully Connected layer for dimension reduction of BERT outputs to num_classes\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_outputs.pooler_output  # Get hidden state (value) of the [CLS] token\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Function that trains the model\n",
        "def train(model, data_loader, optimizer, scheduler, device):\n",
        "    model.train()   # Enable training mode\n",
        "\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()   # Clear gradients\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)   \n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()     # Backpropagation\n",
        "        optimizer.step()    # Update weights\n",
        "        scheduler.step()    # Update learning rate\n",
        "\n",
        "\n",
        "# Function that evaluates the model\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()    # Enable evaluation mode\n",
        "\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "\n",
        "    with torch.no_grad():   # Disable gradient calculation\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)    # Get index of predicted class\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions, zero_division=0)\n",
        "\n",
        "\n",
        "# Function that predicts a program (i.e inputs a given program into the model), also returns confidence level\n",
        "def predict_program(program, model, tokenizer, device, max_length=128):\n",
        "    model.eval()    # Enable evaluation mode\n",
        "\n",
        "    encoding = tokenizer(program, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)    # Tokenize\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():   # Disable gradient calculation\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Get predicted class\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        predicted_program = num2label[preds.item()]\n",
        "\n",
        "        # Calculate confidence\n",
        "        outputs = torch.softmax(logits, dim = 1)\n",
        "        confidence = torch.max(outputs, dim=1)[0].item()*100\n",
        "        \n",
        "    return predicted_program, confidence\n",
        "\n",
        "# Function to check and correct spelling\n",
        "def correct_spelling(text):\n",
        "\n",
        "    # Initialize the spell checker with the custom dictionary\n",
        "    spellcheck = SpellChecker()\n",
        "    spellcheck.word_frequency.load_text_file('corpus.txt')\n",
        "\n",
        "    corrected_text = []\n",
        "    \n",
        "    for word in text.split():\n",
        "        if word in spellcheck:\n",
        "            corrected_text.append(word)\n",
        "        else:\n",
        "            corrected_text.append(spellcheck.correction(word))\n",
        "\n",
        "    # Fix NoneType errors\n",
        "    for index, word in enumerate(corrected_text):\n",
        "        if word is None:\n",
        "            corrected_text[index] = text.split()[index]\n",
        "\n",
        "    return ' '.join(corrected_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "dLBPL84VeUl8"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = len(label2num)\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "num_epochs = 6\n",
        "learning_rate = 2e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "sBFu5EA5eYdJ"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "wg6D_FyUfKRj"
      },
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(programs, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = ProgramClassificationDataset(X_train, y_train, tokenizer, max_length)\n",
        "test_dataset = ProgramClassificationDataset(X_test, y_test, tokenizer, max_length)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "kPRH4FsKexrx"
      },
      "outputs": [],
      "source": [
        "# Instantiate model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3__-BqEGe4p2",
        "outputId": "342f17b4-584e-4150-d85e-3a14a3e46bb6"
      },
      "outputs": [],
      "source": [
        "# Optimizer and Scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufEVrsZXhxnb",
        "outputId": "0a76db10-d024-4cf5-9ddd-abf30c070e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "Accuracy: 0.7421\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      1.00      0.62        16\n",
            "           1       0.00      0.00      0.00         4\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.00      0.00      0.00         6\n",
            "           4       0.00      0.00      0.00         3\n",
            "           5       0.00      0.00      0.00         6\n",
            "           6       0.93      1.00      0.96        13\n",
            "           7       1.00      0.58      0.73        19\n",
            "           8       0.94      1.00      0.97        15\n",
            "           9       1.00      0.86      0.92         7\n",
            "          10       1.00      0.71      0.83        35\n",
            "          11       0.75      1.00      0.86        12\n",
            "          12       0.76      0.91      0.83        35\n",
            "          13       0.73      1.00      0.85        11\n",
            "          14       0.00      0.00      0.00         7\n",
            "          15       1.00      0.93      0.96        14\n",
            "          16       1.00      0.62      0.77        24\n",
            "          17       0.96      1.00      0.98        22\n",
            "          18       0.00      0.00      0.00        14\n",
            "          19       0.68      1.00      0.81        81\n",
            "          20       0.94      1.00      0.97        16\n",
            "          21       0.92      0.88      0.90        25\n",
            "          22       0.00      0.00      0.00         6\n",
            "          23       1.00      1.00      1.00        10\n",
            "          24       0.00      0.00      0.00         8\n",
            "          25       0.78      1.00      0.88         7\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       0.82      1.00      0.90        27\n",
            "          28       1.00      1.00      1.00         8\n",
            "          29       1.00      1.00      1.00        19\n",
            "          30       0.00      0.00      0.00         2\n",
            "          31       1.00      1.00      1.00         5\n",
            "          32       0.00      0.00      0.00         4\n",
            "          34       0.00      0.00      0.00        14\n",
            "          35       0.00      0.00      0.00        10\n",
            "          36       1.00      0.56      0.71         9\n",
            "          37       1.00      0.73      0.84        11\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.36      1.00      0.53        43\n",
            "          40       0.71      0.87      0.78        31\n",
            "          41       0.00      0.00      0.00        14\n",
            "          42       0.00      0.00      0.00         4\n",
            "          43       1.00      0.89      0.94        28\n",
            "          44       1.00      1.00      1.00         6\n",
            "          45       1.00      0.44      0.62         9\n",
            "          46       1.00      1.00      1.00         9\n",
            "          47       0.00      0.00      0.00         7\n",
            "          48       0.00      0.00      0.00         4\n",
            "          49       0.00      0.00      0.00        14\n",
            "          50       0.79      1.00      0.88        23\n",
            "          51       0.00      0.00      0.00        25\n",
            "          52       0.00      0.00      0.00         5\n",
            "          54       0.00      0.00      0.00         2\n",
            "          55       1.00      0.33      0.50        18\n",
            "          56       0.71      1.00      0.83        20\n",
            "          57       1.00      1.00      1.00         6\n",
            "          60       1.00      0.62      0.76        13\n",
            "          61       1.00      1.00      1.00         9\n",
            "          62       1.00      1.00      1.00        13\n",
            "          64       0.89      0.89      0.89         9\n",
            "          65       1.00      1.00      1.00        23\n",
            "          66       0.00      0.00      0.00         1\n",
            "          67       0.39      1.00      0.56         9\n",
            "          68       0.66      1.00      0.79        67\n",
            "\n",
            "    accuracy                           0.74       950\n",
            "   macro avg       0.55      0.56      0.54       950\n",
            "weighted avg       0.67      0.74      0.68       950\n",
            "\n",
            "Epoch 2/6\n",
            "Accuracy: 0.9474\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        16\n",
            "           1       0.00      0.00      0.00         4\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      0.67      0.80         6\n",
            "           4       1.00      1.00      1.00         3\n",
            "           5       1.00      0.83      0.91         6\n",
            "           6       0.93      1.00      0.96        13\n",
            "           7       1.00      1.00      1.00        19\n",
            "           8       0.94      1.00      0.97        15\n",
            "           9       1.00      0.86      0.92         7\n",
            "          10       1.00      0.91      0.96        35\n",
            "          11       0.71      1.00      0.83        12\n",
            "          12       0.97      0.91      0.94        35\n",
            "          13       1.00      1.00      1.00        11\n",
            "          14       0.00      0.00      0.00         7\n",
            "          15       0.88      1.00      0.93        14\n",
            "          16       0.96      1.00      0.98        24\n",
            "          17       1.00      1.00      1.00        22\n",
            "          18       1.00      1.00      1.00        14\n",
            "          19       0.98      0.99      0.98        81\n",
            "          20       1.00      1.00      1.00        16\n",
            "          21       0.93      1.00      0.96        25\n",
            "          22       1.00      0.83      0.91         6\n",
            "          23       1.00      1.00      1.00        10\n",
            "          24       0.00      0.00      0.00         8\n",
            "          25       1.00      1.00      1.00         7\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       1.00      1.00      1.00        27\n",
            "          28       1.00      1.00      1.00         8\n",
            "          29       1.00      1.00      1.00        19\n",
            "          30       1.00      1.00      1.00         2\n",
            "          31       1.00      1.00      1.00         5\n",
            "          32       1.00      1.00      1.00         4\n",
            "          34       1.00      1.00      1.00        14\n",
            "          35       1.00      1.00      1.00        10\n",
            "          36       1.00      1.00      1.00         9\n",
            "          37       1.00      1.00      1.00        11\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.76      0.98      0.86        43\n",
            "          40       0.97      0.97      0.97        31\n",
            "          41       1.00      1.00      1.00        14\n",
            "          42       1.00      1.00      1.00         4\n",
            "          43       0.78      1.00      0.88        28\n",
            "          44       0.86      1.00      0.92         6\n",
            "          45       1.00      1.00      1.00         9\n",
            "          46       1.00      1.00      1.00         9\n",
            "          47       1.00      1.00      1.00         7\n",
            "          48       0.00      0.00      0.00         4\n",
            "          49       1.00      1.00      1.00        14\n",
            "          50       1.00      1.00      1.00        23\n",
            "          51       0.86      1.00      0.93        25\n",
            "          52       1.00      0.60      0.75         5\n",
            "          54       1.00      1.00      1.00         2\n",
            "          55       1.00      0.89      0.94        18\n",
            "          56       1.00      1.00      1.00        20\n",
            "          57       1.00      1.00      1.00         6\n",
            "          60       1.00      1.00      1.00        13\n",
            "          61       1.00      1.00      1.00         9\n",
            "          62       0.81      1.00      0.90        13\n",
            "          63       0.00      0.00      0.00         0\n",
            "          64       1.00      1.00      1.00         9\n",
            "          65       1.00      1.00      1.00        23\n",
            "          66       0.00      0.00      0.00         1\n",
            "          67       1.00      1.00      1.00         9\n",
            "          68       1.00      1.00      1.00        67\n",
            "\n",
            "    accuracy                           0.95       950\n",
            "   macro avg       0.85      0.85      0.85       950\n",
            "weighted avg       0.92      0.95      0.93       950\n",
            "\n",
            "Epoch 3/6\n",
            "Accuracy: 0.9716\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        16\n",
            "           1       1.00      0.25      0.40         4\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         6\n",
            "           4       1.00      1.00      1.00         3\n",
            "           5       1.00      1.00      1.00         6\n",
            "           6       1.00      1.00      1.00        13\n",
            "           7       0.95      1.00      0.97        19\n",
            "           8       1.00      1.00      1.00        15\n",
            "           9       1.00      0.86      0.92         7\n",
            "          10       0.97      1.00      0.99        35\n",
            "          11       0.92      1.00      0.96        12\n",
            "          12       0.97      0.91      0.94        35\n",
            "          13       1.00      1.00      1.00        11\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        14\n",
            "          16       0.96      1.00      0.98        24\n",
            "          17       1.00      1.00      1.00        22\n",
            "          18       0.93      1.00      0.97        14\n",
            "          19       0.99      1.00      0.99        81\n",
            "          20       1.00      1.00      1.00        16\n",
            "          21       0.96      0.96      0.96        25\n",
            "          22       1.00      0.83      0.91         6\n",
            "          23       1.00      1.00      1.00        10\n",
            "          24       1.00      0.88      0.93         8\n",
            "          25       1.00      1.00      1.00         7\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       1.00      1.00      1.00        27\n",
            "          28       1.00      1.00      1.00         8\n",
            "          29       1.00      1.00      1.00        19\n",
            "          30       1.00      1.00      1.00         2\n",
            "          31       1.00      1.00      1.00         5\n",
            "          32       1.00      1.00      1.00         4\n",
            "          34       1.00      1.00      1.00        14\n",
            "          35       1.00      1.00      1.00        10\n",
            "          36       1.00      1.00      1.00         9\n",
            "          37       1.00      1.00      1.00        11\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.89      0.98      0.93        43\n",
            "          40       1.00      0.97      0.98        31\n",
            "          41       1.00      1.00      1.00        14\n",
            "          42       1.00      1.00      1.00         4\n",
            "          43       0.97      1.00      0.98        28\n",
            "          44       0.86      1.00      0.92         6\n",
            "          45       1.00      1.00      1.00         9\n",
            "          46       1.00      1.00      1.00         9\n",
            "          47       1.00      1.00      1.00         7\n",
            "          48       0.00      0.00      0.00         4\n",
            "          49       1.00      1.00      1.00        14\n",
            "          50       1.00      1.00      1.00        23\n",
            "          51       0.86      1.00      0.93        25\n",
            "          52       1.00      0.80      0.89         5\n",
            "          54       1.00      1.00      1.00         2\n",
            "          55       1.00      0.89      0.94        18\n",
            "          56       1.00      1.00      1.00        20\n",
            "          57       1.00      1.00      1.00         6\n",
            "          60       1.00      1.00      1.00        13\n",
            "          61       1.00      1.00      1.00         9\n",
            "          62       0.81      1.00      0.90        13\n",
            "          63       0.00      0.00      0.00         0\n",
            "          64       1.00      1.00      1.00         9\n",
            "          65       1.00      1.00      1.00        23\n",
            "          66       1.00      1.00      1.00         1\n",
            "          67       1.00      1.00      1.00         9\n",
            "          68       1.00      1.00      1.00        67\n",
            "\n",
            "    accuracy                           0.97       950\n",
            "   macro avg       0.92      0.91      0.91       950\n",
            "weighted avg       0.96      0.97      0.97       950\n",
            "\n",
            "Epoch 4/6\n",
            "Accuracy: 0.9747\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        16\n",
            "           1       1.00      0.25      0.40         4\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         6\n",
            "           4       1.00      1.00      1.00         3\n",
            "           5       1.00      1.00      1.00         6\n",
            "           6       1.00      1.00      1.00        13\n",
            "           7       0.95      1.00      0.97        19\n",
            "           8       1.00      1.00      1.00        15\n",
            "           9       1.00      0.86      0.92         7\n",
            "          10       0.97      1.00      0.99        35\n",
            "          11       0.92      1.00      0.96        12\n",
            "          12       1.00      0.91      0.96        35\n",
            "          13       1.00      1.00      1.00        11\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        14\n",
            "          16       0.96      1.00      0.98        24\n",
            "          17       1.00      1.00      1.00        22\n",
            "          18       0.93      1.00      0.97        14\n",
            "          19       0.99      0.99      0.99        81\n",
            "          20       1.00      1.00      1.00        16\n",
            "          21       0.92      0.96      0.94        25\n",
            "          22       1.00      0.83      0.91         6\n",
            "          23       1.00      1.00      1.00        10\n",
            "          24       1.00      0.88      0.93         8\n",
            "          25       1.00      1.00      1.00         7\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       1.00      1.00      1.00        27\n",
            "          28       1.00      1.00      1.00         8\n",
            "          29       1.00      1.00      1.00        19\n",
            "          30       1.00      1.00      1.00         2\n",
            "          31       1.00      1.00      1.00         5\n",
            "          32       1.00      1.00      1.00         4\n",
            "          34       1.00      1.00      1.00        14\n",
            "          35       1.00      1.00      1.00        10\n",
            "          36       1.00      1.00      1.00         9\n",
            "          37       1.00      1.00      1.00        11\n",
            "          38       1.00      1.00      1.00         3\n",
            "          39       0.89      0.98      0.93        43\n",
            "          40       1.00      0.97      0.98        31\n",
            "          41       1.00      1.00      1.00        14\n",
            "          42       1.00      1.00      1.00         4\n",
            "          43       0.96      0.96      0.96        28\n",
            "          44       0.86      1.00      0.92         6\n",
            "          45       1.00      1.00      1.00         9\n",
            "          46       1.00      1.00      1.00         9\n",
            "          47       1.00      1.00      1.00         7\n",
            "          48       0.00      0.00      0.00         4\n",
            "          49       1.00      1.00      1.00        14\n",
            "          50       1.00      1.00      1.00        23\n",
            "          51       0.86      1.00      0.93        25\n",
            "          52       1.00      1.00      1.00         5\n",
            "          54       1.00      1.00      1.00         2\n",
            "          55       0.94      0.94      0.94        18\n",
            "          56       1.00      1.00      1.00        20\n",
            "          57       1.00      1.00      1.00         6\n",
            "          60       1.00      1.00      1.00        13\n",
            "          61       1.00      1.00      1.00         9\n",
            "          62       1.00      1.00      1.00        13\n",
            "          64       1.00      1.00      1.00         9\n",
            "          65       1.00      1.00      1.00        23\n",
            "          66       1.00      1.00      1.00         1\n",
            "          67       1.00      1.00      1.00         9\n",
            "          68       1.00      1.00      1.00        67\n",
            "\n",
            "    accuracy                           0.97       950\n",
            "   macro avg       0.95      0.95      0.94       950\n",
            "weighted avg       0.97      0.97      0.97       950\n",
            "\n",
            "Epoch 5/6\n",
            "Accuracy: 0.9800\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        16\n",
            "           1       1.00      0.25      0.40         4\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         6\n",
            "           4       1.00      1.00      1.00         3\n",
            "           5       1.00      1.00      1.00         6\n",
            "           6       1.00      1.00      1.00        13\n",
            "           7       0.95      1.00      0.97        19\n",
            "           8       1.00      1.00      1.00        15\n",
            "           9       1.00      1.00      1.00         7\n",
            "          10       1.00      1.00      1.00        35\n",
            "          11       0.92      1.00      0.96        12\n",
            "          12       1.00      0.91      0.96        35\n",
            "          13       1.00      1.00      1.00        11\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        14\n",
            "          16       0.96      1.00      0.98        24\n",
            "          17       1.00      1.00      1.00        22\n",
            "          18       0.93      1.00      0.97        14\n",
            "          19       0.99      0.99      0.99        81\n",
            "          20       1.00      1.00      1.00        16\n",
            "          21       0.92      0.96      0.94        25\n",
            "          22       0.83      0.83      0.83         6\n",
            "          23       1.00      1.00      1.00        10\n",
            "          24       1.00      0.88      0.93         8\n",
            "          25       1.00      1.00      1.00         7\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       1.00      1.00      1.00        27\n",
            "          28       1.00      1.00      1.00         8\n",
            "          29       1.00      1.00      1.00        19\n",
            "          30       1.00      1.00      1.00         2\n",
            "          31       1.00      1.00      1.00         5\n",
            "          32       1.00      1.00      1.00         4\n",
            "          34       1.00      1.00      1.00        14\n",
            "          35       1.00      1.00      1.00        10\n",
            "          36       1.00      1.00      1.00         9\n",
            "          37       1.00      1.00      1.00        11\n",
            "          38       1.00      1.00      1.00         3\n",
            "          39       0.98      0.98      0.98        43\n",
            "          40       1.00      0.97      0.98        31\n",
            "          41       1.00      1.00      1.00        14\n",
            "          42       1.00      1.00      1.00         4\n",
            "          43       0.96      0.96      0.96        28\n",
            "          44       0.86      1.00      0.92         6\n",
            "          45       1.00      1.00      1.00         9\n",
            "          46       1.00      1.00      1.00         9\n",
            "          47       1.00      1.00      1.00         7\n",
            "          48       1.00      0.75      0.86         4\n",
            "          49       1.00      1.00      1.00        14\n",
            "          50       1.00      1.00      1.00        23\n",
            "          51       0.86      1.00      0.93        25\n",
            "          52       1.00      1.00      1.00         5\n",
            "          54       1.00      1.00      1.00         2\n",
            "          55       1.00      1.00      1.00        18\n",
            "          56       1.00      1.00      1.00        20\n",
            "          57       1.00      1.00      1.00         6\n",
            "          60       1.00      1.00      1.00        13\n",
            "          61       1.00      1.00      1.00         9\n",
            "          62       1.00      1.00      1.00        13\n",
            "          64       1.00      1.00      1.00         9\n",
            "          65       1.00      1.00      1.00        23\n",
            "          66       1.00      1.00      1.00         1\n",
            "          67       1.00      1.00      1.00         9\n",
            "          68       1.00      1.00      1.00        67\n",
            "\n",
            "    accuracy                           0.98       950\n",
            "   macro avg       0.97      0.96      0.96       950\n",
            "weighted avg       0.98      0.98      0.98       950\n",
            "\n",
            "Epoch 6/6\n",
            "Accuracy: 0.9853\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89        16\n",
            "           1       1.00      0.25      0.40         4\n",
            "           2       1.00      1.00      1.00         5\n",
            "           3       1.00      1.00      1.00         6\n",
            "           4       1.00      1.00      1.00         3\n",
            "           5       1.00      1.00      1.00         6\n",
            "           6       1.00      1.00      1.00        13\n",
            "           7       0.95      1.00      0.97        19\n",
            "           8       1.00      1.00      1.00        15\n",
            "           9       1.00      1.00      1.00         7\n",
            "          10       1.00      1.00      1.00        35\n",
            "          11       0.92      1.00      0.96        12\n",
            "          12       1.00      0.91      0.96        35\n",
            "          13       1.00      1.00      1.00        11\n",
            "          14       1.00      1.00      1.00         7\n",
            "          15       1.00      1.00      1.00        14\n",
            "          16       0.96      1.00      0.98        24\n",
            "          17       1.00      1.00      1.00        22\n",
            "          18       0.93      1.00      0.97        14\n",
            "          19       0.99      0.99      0.99        81\n",
            "          20       1.00      1.00      1.00        16\n",
            "          21       0.92      0.96      0.94        25\n",
            "          22       0.83      0.83      0.83         6\n",
            "          23       1.00      1.00      1.00        10\n",
            "          24       1.00      0.88      0.93         8\n",
            "          25       1.00      1.00      1.00         7\n",
            "          26       1.00      0.80      0.89         5\n",
            "          27       1.00      1.00      1.00        27\n",
            "          28       1.00      1.00      1.00         8\n",
            "          29       1.00      1.00      1.00        19\n",
            "          30       1.00      1.00      1.00         2\n",
            "          31       1.00      1.00      1.00         5\n",
            "          32       1.00      1.00      1.00         4\n",
            "          34       1.00      1.00      1.00        14\n",
            "          35       1.00      1.00      1.00        10\n",
            "          36       1.00      1.00      1.00         9\n",
            "          37       1.00      1.00      1.00        11\n",
            "          38       1.00      1.00      1.00         3\n",
            "          39       1.00      0.98      0.99        43\n",
            "          40       1.00      0.97      0.98        31\n",
            "          41       1.00      1.00      1.00        14\n",
            "          42       1.00      1.00      1.00         4\n",
            "          43       0.96      0.96      0.96        28\n",
            "          44       0.86      1.00      0.92         6\n",
            "          45       1.00      1.00      1.00         9\n",
            "          46       1.00      1.00      1.00         9\n",
            "          47       1.00      1.00      1.00         7\n",
            "          48       1.00      1.00      1.00         4\n",
            "          49       1.00      1.00      1.00        14\n",
            "          50       1.00      1.00      1.00        23\n",
            "          51       1.00      1.00      1.00        25\n",
            "          52       1.00      1.00      1.00         5\n",
            "          54       1.00      1.00      1.00         2\n",
            "          55       1.00      1.00      1.00        18\n",
            "          56       1.00      1.00      1.00        20\n",
            "          57       1.00      1.00      1.00         6\n",
            "          60       1.00      1.00      1.00        13\n",
            "          61       1.00      1.00      1.00         9\n",
            "          62       1.00      1.00      1.00        13\n",
            "          64       1.00      1.00      1.00         9\n",
            "          65       1.00      1.00      1.00        23\n",
            "          66       1.00      1.00      1.00         1\n",
            "          67       1.00      1.00      1.00         9\n",
            "          68       1.00      1.00      1.00        67\n",
            "\n",
            "    accuracy                           0.99       950\n",
            "   macro avg       0.99      0.98      0.98       950\n",
            "weighted avg       0.99      0.99      0.98       950\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    train(model, train_dataloader, optimizer, scheduler, device)\n",
        "    accuracy, report = evaluate(model, test_dataloader, device)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "UaG-WeCWkgio"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"bert_program_classifier.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load weights\n",
        "model.load_state_dict(torch.load(\"bert_program_classifier.pth\"), True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "JDCjTQqIklIa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input program: Integrated Biomedical Engineering & Health Sciences\n",
            "Predicted category: biomedical engineering\n",
            "Confidence: 54.99767065048218\n"
          ]
        }
      ],
      "source": [
        "# Test prediction\n",
        "test_text = \"Integrated Biomedical Engineering & Health Sciences\"\n",
        "corrected_text = correct_spelling(test_text)\n",
        "predicted_label = predict_program(corrected_text, model, tokenizer, device)\n",
        "print(f\"Input program: {test_text}\")\n",
        "print(f\"Predicted category: {predicted_label[0]}\")\n",
        "print(f\"Confidence: {predicted_label[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "D2VkK40jk2e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input program: computer science\n",
            "Predicted category: computer science\n",
            "Confidence: 97.23633527755737\n"
          ]
        }
      ],
      "source": [
        "# Test prediction\n",
        "test_text = \"computer science\"\n",
        "corrected_text = correct_spelling(test_text)\n",
        "predicted_label = predict_program(corrected_text, model, tokenizer, device)\n",
        "print(f\"Input program: {test_text}\")\n",
        "print(f\"Predicted category: {predicted_label[0]}\")\n",
        "print(f\"Confidence: {predicted_label[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "sz7F07-8lGor"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input program: computer engineering\n",
            "Predicted category: computer engineering\n",
            "Confidence: 88.51482272148132\n"
          ]
        }
      ],
      "source": [
        "# Test prediction\n",
        "test_text = \"computer engineering\"\n",
        "corrected_text = correct_spelling(test_text)\n",
        "predicted_label = predict_program(corrected_text, model, tokenizer, device)\n",
        "print(f\"Input program: {test_text}\")\n",
        "print(f\"Predicted category: {predicted_label[0]}\")\n",
        "print(f\"Confidence: {predicted_label[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "OSLlArSqlIOi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input program: computer eng\n",
            "Predicted category: computer engineering\n",
            "Confidence: 67.91397333145142\n"
          ]
        }
      ],
      "source": [
        "# Test prediction\n",
        "test_text = \"computer eng\"\n",
        "corrected_text = correct_spelling(test_text)\n",
        "predicted_label = predict_program(corrected_text, model, tokenizer, device)\n",
        "print(f\"Input program: {test_text}\")\n",
        "print(f\"Predicted category: {predicted_label[0]}\")\n",
        "print(f\"Confidence: {predicted_label[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "k8Kz60x_lENB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input program: comp sci\n",
            "Predicted category: computer science\n",
            "Confidence: 94.17394399642944\n"
          ]
        }
      ],
      "source": [
        "# Test prediction\n",
        "test_text = \"comp sci\"\n",
        "corrected_text = correct_spelling(test_text)\n",
        "predicted_label = predict_program(corrected_text, model, tokenizer, device)\n",
        "print(f\"Input program: {test_text}\")\n",
        "print(f\"Predicted category: {predicted_label[0]}\")\n",
        "print(f\"Confidence: {predicted_label[1]}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
